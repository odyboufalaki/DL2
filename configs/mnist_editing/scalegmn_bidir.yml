batch_size: 64
data:
  dataset: mnist_inr_edit
  dataset_path: ./data/mnist-inrs/
  split_path: ./data/mnist-inrs/mnist_splits.json  #
  image_size: [28,28]
  img_ds: MNIST
  style_function: dilate
  node_pos_embed: &node_pos_embed True
  edge_pos_embed: &edge_pos_embed False
  # the below can be extracted per datapoint, but since it is the same for all, we can define it here
  layer_layout: [2, 32, 32, 1]
  switch_to_canon: True

inr_model:
  in_features: 2
  out_features: 1
  n_layers: 3
  hidden_features: 32
  img_shape: [28,28]

train_args:
  num_epochs: 300
  val_acc_threshold: 0.1
  patience: 50
  seed: 0
  loss: MSE
  eval_every: 1000


scalegmn_args:
  d_in_v: &d_in_v 1  # initial dimension of input nn bias
  d_in_e: &d_in_e 1  # initial dimension of input nn weights
  d_hid: &d_hid 128  # hidden dimension
  num_layers: 10 # number of gnn layers to apply
  direction: bidirectional
  equivariant: True
  symmetry: sign  # symmetry
  jit: False # prefer compile - compile gnn to optimize performance
  compile: False # compile gnn to optimize performance

  out_scale: 0.01
  gnn_skip_connections: True

  concat_mlp_directions: False  # only for bidirectional: apply an MLP before concatenating the forward and backward directions
  reciprocal: True

  node_pos_embed: *node_pos_embed  # use positional encodings
  edge_pos_embed: *edge_pos_embed  # use positional encodings

  graph_init:
    d_in_v: *d_in_v
    d_in_e: *d_in_e
    project_node_feats: True
    project_edge_feats: True
    d_node: *d_hid
    d_edge: *d_hid
    
  positional_encodings:
    final_linear_pos_embed: False
    sum_pos_enc: False
    po_as_different_linear: False
    equiv_net: False
    # args for the equiv net option.
    sum_on_io: True
    equiv_on_hidden: True
    num_mlps: 3
    layer_equiv_on_hidden: False

  gnn_args:
    d_hid: *d_hid
    message_fn_layers: 1
    message_fn_skip_connections: False
    update_node_feats_fn_layers: 1
    update_node_feats_fn_skip_connections: False
    update_edge_attr: True
    dropout: 0.1
    dropout_all: False
    update_as_act: False
    update_as_act_arg: sum
    mlp_on_io: True

    msg_equiv_on_hidden: True
    upd_equiv_on_hidden: True
    layer_msg_equiv_on_hidden: False
    layer_upd_equiv_on_hidden: False
    msg_num_mlps: 2
    upd_num_mlps: 2
    pos_embed_msg: False
    pos_embed_upd: False
    layer_norm: False
    aggregator: add
    sign_symmetrization: False

  mlp_args:
    d_k: [ *d_hid ]
    activation: silu
    dropout: 0.
    final_activation: identity
    batch_norm: False
    layer_norm: True
    bias: True
    skip: False


optimization:
  clip_grad: True
  clip_grad_max_norm: 10.0
  optimizer_name: AdamW
  optimizer_args:
    lr: 0.001
    weight_decay: 0.001
  scheduler_args:
    scheduler: WarmupLRScheduler
    warmup_steps: 1000
    scheduler_mode: min
    decay_rate: 0
    decay_steps: 0
    patience: None
    min_lr: None

log_n_imgs: 4
wandb_args:
  project: mnist_editing
  entity: null
  group: null
  name: null
  tags: null