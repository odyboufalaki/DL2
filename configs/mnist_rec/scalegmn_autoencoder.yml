batch_size: 128
data:
  dataset: labeled_mnist_inr
  dataset_path: ./data/mnist-inrs/
  split_path: ./data/mnist-inrs/mnist_splits.json  #
  image_size: [28,28]
  node_pos_embed: &node_pos_embed True
  edge_pos_embed: &edge_pos_embed True
  # the below can be extracted per datapoint, but since it is the same for all, we can define it here
  layer_layout: &layer_layout [2, 32, 32, 1]
  switch_to_canon: True

train_args:
  num_epochs: 100
  val_acc_threshold: 0.1
  patience: 50
  seed: 0
  loss: ReconstructionLoss
  weigthed_loss: False  # Weighted loss assigns a weight proportional to the pixel value
  reconstruction_type: pixels # pixels or inr
  pixel_expansion: 1 # Pixel granularity for the reconstruction loss and generated images.

scalegmn_args:
  d_in_v: &d_in_v 1  # initial dimension of input nn bias
  d_in_e: &d_in_e 1  # initial dimension of input nn weights
  d_hid: &d_hid 32  # hidden dimension
  num_layers: 4 # number of gnn layers to apply
  direction: forward
  equivariant: False
  symmetry: sign  # symmetry
  jit: False # prefer compile - compile gnn to optimize performance
  compile: False # compile gnn to optimize performance

  readout_range: full_graph  # or full_graph
  gnn_skip_connections: True

  concat_mlp_directions: False  # only for bidirectional: apply an MLP before concatenating the forward and backward directions
  reciprocal: True  # only for bidirectional

  node_pos_embed: *node_pos_embed  # use positional encodings
  edge_pos_embed: *edge_pos_embed  # use positional encodings

  graph_init:
    d_in_v: *d_in_v
    d_in_e: *d_in_e
    project_node_feats: True
    project_edge_feats: True
    d_node: *d_hid
    d_edge: *d_hid
    
  positional_encodings:
    final_linear_pos_embed: False
    sum_pos_enc: False
    po_as_different_linear: False
    equiv_net: False
    # args for the equiv net option.
    sum_on_io: True
    equiv_on_hidden: True
    num_mlps: 3
    layer_equiv_on_hidden: False

  gnn_args:
    d_hid: *d_hid
    message_fn_layers: 1
    message_fn_skip_connections: False
    update_node_feats_fn_layers: 1
    update_node_feats_fn_skip_connections: False
    update_edge_attr: True
    dropout: 0.2
    dropout_all: False  # False: only in between the gnn layers, True: + all mlp layers
    update_as_act: False
    update_as_act_arg: sum
    mlp_on_io: False

    msg_equiv_on_hidden: True
    upd_equiv_on_hidden: True
    layer_msg_equiv_on_hidden: False
    layer_upd_equiv_on_hidden: False
    msg_num_mlps: 2
    upd_num_mlps: 2
    pos_embed_msg: False
    pos_embed_upd: False
    layer_norm: False
    aggregator: add
    sign_symmetrization: True

  mlp_args:
    d_k: [ *d_hid ]
    activation: silu
    dropout: 0.
    final_activation: identity
    batch_norm: False
    layer_norm: True
    bias: True
    skip: False

  readout_args:
    d_out: *d_hid  # final dimension of readout Permutation Invariant Scale(Sign)Net - Number of Classes for classification tasks.
    d_rho: *d_hid  # intermediate dimension within Readout module

decoder_args:
  data_layer_layout: *layer_layout  # explicitly define the layer layout
  d_input: *d_hid  # initial dimension of input nn bias
  d_hidden: []  # hidden dimension
  activation: silu  # activation function

optimization:
  clip_grad: True
  clip_grad_max_norm: 10.0
  optimizer_name: AdamW
  optimizer_args:
    lr: 5e-1
    weight_decay: 0.01
  scheduler_args:
    scheduler: WarmupLRScheduler
    warmup_steps: 2000
    scheduler_mode: min
    decay_rate: 0
    decay_steps: 0
    patience: None
    min_lr: None

wandb_args:
  project: mnist_cls
  entity: null
  group: null
  name: null
  tags: null

save_model:
  save_model: True
  save_dir: ./models/mnist_cls/scalegmn_autoencoder
  save_name: scalegmn_autoencoder_mnist_cls.pt
  save_best_only: True
  save_last_only: False